from transformers import PreTrainedTokenizerFast

token_ids = [200006, 17360, 200008, 3575, 553, 17554, 162016, 11, 261, 4410, 6439, 2359, 22203, 656, 7788, 17527, 558, 87447, 100594, 25, 220, 1323, 19, 12, 3218, 198, 6576, 3521, 25, 220, 1323, 20, 12, 994, 12, 3000, 279, 30377, 289, 25, 1932, 279, 2, 13888, 18403, 25, 8450, 11, 1721, 13, 21030, 2804, 413, 7360, 395, 1753, 3176, 13, 200007, 200006, 77944, 200008, 200007, 200006, 77944, 200008, 198, 309, 1877, 261, 59245, 8333, 11, 634, 5296, 382, 316, 4218, 290, 3100, 326, 3587, 198, 309, 290, 31111, 11736, 306, 5701, 484, 3981, 290, 3992, 5701, 56952, 35549, 309, 405, 220, 392, 35913, 1243, 405, 271, 392, 83, 1243, 405, 530, 392, 4576, 1243, 392, 51, 1150, 530, 392, 2493, 1243, 392, 27378, 1092, 271, 606, 220, 1862, 220, 392, 12919, 1243, 4240, 271, 392, 83, 1092, 220, 6128, 220, 392, 4576, 1243, 392, 17045, 1150, 220, 392, 2493, 1243, 392, 3369, 1092, 943, 309, 9393, 3239, 316, 622, 448, 4934, 328, 290, 8205, 11, 625, 290, 19581, 8807, 198, 200007, 200006, 1428, 200008, 4827, 553, 481, 30, 200007, 200006, 173781]

tokenizer = PreTrainedTokenizerFast.from_pretrained(pretrained_model_name_or_path="openai/gpt-oss-20b")

print(tokenizer.decode(token_ids))